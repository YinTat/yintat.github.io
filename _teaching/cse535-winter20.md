---
layout: single
title: "CSE 535: Theory of Optimization and Continuous Algorithms"
permalink: /teaching/cse535-winter20/
author_profile: false
---

# CSE 535: Theory of Optimization and Continuous Algorithms

The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is derived from this by appropriate discretization. In interesting and general settings, the current fastest methods are a consequence of this perspective. We will discuss several examples of algorithms for high-dimensional optimization, and use them to understand the following concepts in detail.
+ Elimination
+ Reduction
+ Geometrization
+ Sparsification
+ Acceleration
+ Decomposition

This course is offered in Georgia Tech at the same time by [Santosh Vempala](https://santoshv.github.io/2020CS6550/contalgos.html).


## Administrative Information:
+ Instructor: Yin Tat Lee
+ Office Hours: By appointment, email me at yintat@<span style="display: none;">ignoreme-</span>uw.edu
+ TA Office hours: Fri 2:45-4:00, CSE 021
+ Lectures: Tue, Thu 10:00-11:20 at ARC G070
+ Text Book (in progress): [https://www.dropbox.com/s/10zwtzolc1qhpsq/main.pdf](https://www.dropbox.com/s/10zwtzolc1qhpsq/main.pdf)
+ Course evaluation: Homework (100%)
+ Prerequisite: basic knowledge of algorithms, probability, linear algebra.

## Free Consulting Service (only for students)

If you have any difficult optimization problems in your research, feel free to ask me. Similarly, if you have done some amazing optimization research and want to tell me, do not hesitate. (I am not an expert on optimization in practice though. So, do not set a high expectation.)

Email me for an appointment.


## Assignments

Submitted via [Gradescope](https://www.gradescope.com/courses/35189). Check emails.

## Tentative Schedule:

### Introduction
+ Jan 07: Introduction (1.1, 1.2, 1.3)
+ Jan 09: Gradient Descent (1.5, 1.6)

### Elimination
+ Jan 14: Cutting Plane Methods (2.1, 2.2, 2.3, 2.4)
+ Jan 16: Sphere and Parabola Method (2.6)

### Reduction
+ Jan 21: Equivalences (3.1, 3.2, 3.3, 3.4)
+ Jan 23: Duality (3.6)

### Geometrization
+ Jan 28: Lower Bounds (4.1)
+ Jan 30: Mirror Descent (4.1)
+ Feb 04: Frank-Wolfe (4.2)
+ Feb 06: Newton Method & L-BFGS (4.3)
+ Feb 11: Interior Point Method (4.4)
+ Feb 13: Interior Point Method (4.4)

### Sparsification
+ Feb 18: Subspace Embedding (6.1)
+ Feb 20: Leverage Score Sampling (6.2)
+ Feb 25: Coordinate Descent (6.4)
+ Feb 27: Stochastic Gradient Descent & Variance Reduction (6.3)

### Acceleration
+ Mar 3: Conjugate Gradient & Chebyshev Expansion (8.1, 8.2)
+ Mar 5: Accelerated Gradient Descent (8.3)

### Decomposition
+ Mar 10: Cholesky Decomposition (???)
+ Mar 12: Laplacian Systems (???)

## Related Courses:
+ [Aaron Sidford, Introduction to Optimization Theory](http://www.aaronsidford.com/sp17_opt_theory.html)
+ [Lap Chi Lau, Convexity and Optimization](https://cs.uwaterloo.ca/~lapchi/cs798/index.html)
+ [Nisheeth Vishnoi, Algorithms for Convex Optimization](https://nisheethvishnoi.wordpress.com/convex-optimization/)
+ [Jonathan Kelner, Topics in Theoretical Computer Science: An Algorithmist's Toolkit](http://stellar.mit.edu/S/course/18/sp14/18.409/index.html)
+ [Santosh Vempala, Simple Algorithms](https://algorithms2017.wordpress.com/lectures/)
+ [Sheehan Olver, Numerical Complex Analysis](http://www.maths.usyd.edu.au/u/olver/teaching/NCA/)
