---
layout: single
title: "CSE 535: Theory of Optimization and Continuous Algorithms"
permalink: /teaching/cse535-winter20/
author_profile: false
---

# CSE 535: Theory of Optimization and Continuous Algorithms

The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is derived from this by appropriate discretization. In interesting and general settings, the current fastest methods are a consequence of this perspective. We will discuss several examples of algorithms for high-dimensional optimization, and use them to understand the following concepts in detail.
+ Elimination
+ Reduction
+ Geometrization
+ Acceleration
+ Sparsification
+ Decomposition

This course is offered in Georgia Tech at the same time by [Santosh Vempala](https://santoshv.github.io/contalgos.html).


## Administrative Information:
+ Instructor: Yin Tat Lee
+ Office Hours: By appointment, email me at yintat@<span style="display: none;">ignoreme-</span>uw.edu
+ TA Office hours: Fri 2:45-4:00, CSE 021
+ Lectures: Tue, Thu 10:00-11:20 at ARC G070
+ Text Book (in progress): [https://www.dropbox.com/s/10zwtzolc1qhpsq/main.pdf](https://www.dropbox.com/s/10zwtzolc1qhpsq/main.pdf)
+ Course evaluation: Homework (100%)
+ Prerequisite: basic knowledge of algorithms, probability, linear algebra.

## Assignments

Submitted via [Gradescope](https://www.gradescope.com/courses/35189). Check emails.

## Tentative Schedule:

### Introduction
+ Jan 07: Introduction (1.1, 1.2, 1.3)
+ Jan 09: Gradient Descent (1.5, 1.6)

### Elimination
+ Jan 14: Limitation of Gradient Descent (2.7)
+ Jan 16: Cutting Plane Methods (2.1, 2.2)
+ Jan 21: Cutting Plane Methods (2.3, 2.4)
+ Jan 23: Sphere and Parabola Method (2.6)

### Reduction
+ Jan 28: Equivalences (3.1, 3.2, 3.3, 3.4)
+ Jan 30: Duality (3.6)

### Geometrization
+ Feb 04: Mirror Descent (4.1)
+ Feb 06: Frank-Wolfe (4.2)
+ Feb 11: Newton Method & L-BFGS (4.3)
+ Feb 13: Interior Point Method (4.4)

### Sparsification
+ Feb 18: Subspace Embedding (6.1)
+ Feb 20: Leverage Score Sampling (6.2)
+ Feb 25: Coordinate Descent (6.4)
+ Feb 27: Stochastic Gradient Descent & Variance Reduction (6.3)

### Linear Systems
+ Mar 3: Cholesky Decomposition (???)
+ Mar 5: Laplacian Systems (???)

### Acceleration
+ Mar 10: Conjugate Gradient & Chebyshev Expansion (8.1, 8.2)
+ Mar 12: Accelerated Gradient Descent (8.3)

## Related Courses:
+ [Aaron Sidford, Introduction to Optimization Theory](http://www.aaronsidford.com/sp17_opt_theory.html)
+ [Lap Chi Lau, Convexity and Optimization](https://cs.uwaterloo.ca/~lapchi/cs798/index.html)
+ [Nisheeth Vishnoi, Algorithms for Convex Optimization](https://nisheethvishnoi.wordpress.com/convex-optimization/)
+ [Jonathan Kelner, Topics in Theoretical Computer Science: An Algorithmist's Toolkit](http://stellar.mit.edu/S/course/18/sp14/18.409/index.html)
+ [Santosh Vempala, Simple Algorithms](https://algorithms2017.wordpress.com/lectures/)
+ [Sheehan Olver, Numerical Complex Analysis](http://www.maths.usyd.edu.au/u/olver/teaching/NCA/)
