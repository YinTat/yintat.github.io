---
layout: single
title: "CSE 535: Theory of Optimization and Continuous Algorithms"
permalink: /teaching/cse535-winter19/
author_profile: false
---

# CSE 535: Theory of Optimization and Continuous Algorithms

The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is derived from this by appropriate discretization. In interesting and general settings, the current fastest methods are a consequence of this perspective. We will discuss several examples of algorithms for high-dimensional optimization and sampling, and use them to understand the following concepts in detail.
+ Elimination
+ Reduction
+ Geometrization
+ Acceleration
+ Sparsification
+ Decomposition

This course is offered in Georgia Tech at the same time by [Santosh Vempala](https://santoshv.github.io/contalgos.html).


## Administrative Information:
+ Instructor: Yin Tat Lee
+ Office Hours: By appointment, email me at yintat@<span style="display: none;">ignoreme-</span>uw.edu
+ Lectures: Tue, Thu 10:00-11:20 at ARC G070
+ Lecture Note: https://www.dropbox.com/s/wdxhrlcnjz3tecj/main.pdf
+ Course evaluation: Homework (100%)
+ Prerequisite: basic knowledge of algorithms, probability, linear algebra.

## Assignments

Submitted via [Gradescope](https://www.gradescope.com/courses/35189). (I will send out the info this week.)
+ Assignment 1 due Monday, 14-Jan 11:59PM. [See](http://yintat.com/pdf/hw1.pdf)
+ Assignment 2 due Monday, 28-Jan 11:59PM
+ Assignment 3 due Monday, 11-Feb 11:59PM
+ Assignment 4 due Monday, 25-Feb 11:59PM
+ Assignment 5 due Monday, 11-Mar 11:59PM

## Tentative Schedule:

### Introduction
+ Jan 08: Gradient Descent (1.1, 1.2, 1.4)
+ Jan 10: Langevin Dynamics (1.3, 1.5, 1.6)

### Elimination
+ Jan 15: Cutting Plane Methods
+ Jan 17: Geometric Descent

### Reduction
+ Jan 22: Equivalences
+ Jan 24: Duality

### Geometrization (Optimization)
+ Jan 29: Mirror Descent & Frank-Wolfe
+ Jan 31: Newton Method & L-BFGS
+ Feb 05: Interior Point Method
+ Feb 07: Applications

### Geometrization (Sampling)
+ Feb 12: Ball walk & Isoperimetry
+ Feb 14: Isotropic Transformation & Simulated Annealing.
+ Feb 19: Hit-and-Run, Dikin walk
+ Feb 21: RHMC

### Acceleration
+ Feb 26: Chebyshev Expansion & Conjugate Gradient 
+ Feb 28: Accelerated Gradient Descent

### Sparsification
+ Mar 5: Stochastic Gradient Descent & Variance Reduction
+ Mar 7: Leverage Score Sampling

### Decomposition
+ Mar 12: Cholesky decomposition
+ Mar 14: Laplacian Solver

## Related Theory Optimization Courses:
+ [Aaron Sidford](http://www.aaronsidford.com/sp17_opt_theory.html)
+ [Lap Chi Lau](https://cs.uwaterloo.ca/~lapchi/cs798/index.html)
+ [Nisheeth Vishnoi](https://nisheethvishnoi.wordpress.com/convex-optimization/)
+ [Jonathan Kelner](http://stellar.mit.edu/S/course/18/sp14/18.409/index.html)
+ [Aleksander MÄ…dry](http://courses.csail.mit.edu/6.S978/)
+ [Santosh Vempala](https://algorithms2017.wordpress.com/lectures/)
