---
layout: single
title: "CSE 535: Theory of Optimization and Continuous Algorithms"
permalink: /teaching/cse535-winter19/
author_profile: false
---

# CSE 535: Theory of Optimization and Continuous Algorithms

The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is derived from this by appropriate discretization. In interesting and general settings, the current fastest methods are a consequence of this perspective. We will discuss several examples of algorithms for high-dimensional optimization and sampling, and use them to understand the following concepts in detail.
+ Elimination
+ Reduction
+ Geometrization
+ Acceleration
+ Sparsification
+ Decomposition

This course is offered in Georgia Tech at the same time by [Santosh Vempala](https://santoshv.github.io/contalgos.html).


## Administrative Information:
+ Instructor: Yin Tat Lee
+ Office Hours: By appointment, email me at yintat@<span style="display: none;">ignoreme-</span>uw.edu
+ TA Office hours: Fri 2:45-4:00, CSE 021
+ Lectures: Tue, Thu 10:00-11:20 at ARC G070
+ Lecture Note: [https://www.dropbox.com/s/wdxhrlcnjz3tecj/main.pdf](https://www.dropbox.com/s/wdxhrlcnjz3tecj/main.pdf)
+ Course evaluation: Homework (100%)
+ Prerequisite: basic knowledge of algorithms, probability, linear algebra.

## Assignments

Submitted via [Gradescope](https://www.gradescope.com/courses/35189). Check emails.

## Tentative Schedule:

### Introduction
+ Jan 08: Gradient Descent (1.1, 1.2, 1.4)
+ Jan 10: Langevin Dynamics (1.3, 1.5, 1.6)

### Elimination
+ Jan 15: Cutting Plane Methods (2.1, 2.2, 2.3, 2.4)
+ Jan 17: Sphere and Parabola Method (2.6, 2.7)

### Reduction
+ Jan 22: Equivalences (3.1, 3.2, 3.3)
+ Jan 24: Equivalences (3.4)
+ Jan 29: Duality (3.5)

### Geometrization (Optimization)
+ Jan 31: Mirror Descent
+ Feb 05: Frank-Wolfe
+ Feb 07: Newton Method & L-BFGS
+ Feb 12: Interior Point Method

### Geometrization (Sampling)
+ Feb 14: Ball walk & Isoperimetry
+ Feb 19: Isotropic Transformation & Simulated Annealing.
+ Feb 21: Hit-and-Run, Dikin walk
+ Feb 26: RHMC

### Acceleration
+ Feb 28: Chebyshev Expansion & Conjugate Gradient 
+ Mar 5: Accelerated Gradient Descent

### Sparsification
+ Mar 7: Stochastic Gradient Descent & Variance Reduction
+ Mar 12: Leverage Score Sampling

### Decomposition
+ Mar 14: Cholesky decomposition
+ Mar ??: Laplacian Solver (Anyone want extra lecture?)

## Related Theory Optimization Courses:
+ [Aaron Sidford](http://www.aaronsidford.com/sp17_opt_theory.html)
+ [Lap Chi Lau](https://cs.uwaterloo.ca/~lapchi/cs798/index.html)
+ [Nisheeth Vishnoi](https://nisheethvishnoi.wordpress.com/convex-optimization/)
+ [Jonathan Kelner](http://stellar.mit.edu/S/course/18/sp14/18.409/index.html)
+ [Aleksander MÄ…dry](http://courses.csail.mit.edu/6.S978/)
+ [Santosh Vempala](https://algorithms2017.wordpress.com/lectures/)
