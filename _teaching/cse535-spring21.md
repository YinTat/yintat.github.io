---
layout: single
title: "CSE 535: Theory of Optimization and Continuous Algorithms"
permalink: /teaching/cse535-spring21/
author_profile: false
---

# CSE 535: Theory of Optimization and Continuous Algorithms

The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is derived from this by appropriate discretization. In interesting and general settings, the current fastest methods are a consequence of this perspective. We will discuss several examples of algorithms for high-dimensional optimization, and use them to understand the following concepts in detail.
+ Elimination
+ Reduction
+ Geometrization
+ Sparsification
+ Acceleration
+ Decomposition

## Administrative Information:
+ Lectures: Tue, Thu 10:00-11:20 at	[Zoom](https://washington.zoom.us/j/95120229931)
+ Text Book (in progress): [https://www.dropbox.com/s/9ck2p07mibvwrmg/main.pdf](https://www.dropbox.com/s/9ck2p07mibvwrmg/main.pdf).
+ Please do not make comment directly on the merged version, but instead the split versions below. Dropbox cannot handle large documents efficiently.
+ Prerequisite: basic knowledge of algorithms, probability, linear algebra.

## Contacts:
+ Instructor: Yin Tat Lee
+ Office Hours: By appointment, email me at (yintat@<span style="display: none;">ignoreme-</span>uw.edu) 
+ TA: Swati Padmanabhan (pswati@<span style="display: none;">ignoreme-</span>cs.washington.edu)
+ TA Office Hours: 1130 am - 1220 am Thu and 930 pm - 1020 pm Thu, zoom link on edstem.
+ Mailing List: [Here](https://mailman.u.washington.edu/mailman/listinfo/cse535a_sp21)
+ Edstem: [Here](https://edstem.org/us/courses/4940)

## Assignments
+ Course evaluation: 5 assignments (100%) or 1 project (100%)
+ Assignments posted and submitted via [Gradescope](https://www.gradescope.com/courses/259020).

## Class Policy
+ You may discuss assignments with others, but you must write down the solutions by yourself.
+ Assignments as well as regrade requests are to be submitted only via gradescope.
+ In general, late submissions are not accepted (the submission server closes at the deadline). Gradescope lets you overwrite previous submissions, so it is recommended to use this feature to avoid missing the submission deadline altogether.
+ We follow the standard [UW CSE policy](https://www.cs.washington.edu/academics/misconduct) for academic integrity.
+ The office hours are for general questions about material from the lecture and homework problems.
+ Please refer to university policies regarding [disability accommodations](http://depts.washington.edu/uwdrs/current-students/accommodations/) or [religious accommodations](https://registrar.washington.edu/staffandfaculty/religious-accommodations-policy/).


## Tentative Schedule:

### Introduction
+ Mar 30: Convexity [Sec 1.1-1.5 (Updated on 31 Mar)](https://www.dropbox.com/s/jp6kftjej0tukrz/lecture%201.pdf)
+ Apr 01: Convexity and Derivative [Sec 4.1-4.4](https://www.dropbox.com/s/o9wn9d6gok0hi95/lecture%202.pdf) [Reading](https://www.dropbox.com/s/wha9wrfs59hco80/reading%202.pdf?dl=0)

### Basic Methods
+ Apr 06: Gradient Descent [Sec 2 (Added Sec 2.6 on 7 Apr)](https://www.dropbox.com/s/cx6husyj2nbfq2e/lecture%203.pdf?dl=0) 
+ Apr 06: (HW 1 due)
+ Apr 08: Gradient Descent (cont) and Proximal Methods (See Sec 2 above)
+ Apr 13: Cutting Plane Methods [Sec 3](https://www.dropbox.com/s/hgtcajmix8q7qzh/lecture%205.pdf?dl=0)
+ Apr 15: Cutting Plane Methods (cont)

### Geometrization
+ Apr 20: Mirror Descent (HW 2 due)
+ Apr 22: Newton Method & L-BFGS

### Homotopy Method
+ Apr 27: Interior Point Method
+ Apr 29: Robust Interior Point Method

### Sparsification
+ May 04: Leverage Score Sampling (HW 3 due or project proposal due)
+ May 06: Stochastic Gradient Descent & Variance Reduction

### Combinatorial Methods
+ May 11: Cholesky Decomposition
+ May 13: Laplacian Solver

### Acceleration
+ May 18: Conjugate Gradient & Chebyshev Expansion (HW 4 due)
+ May 20: Accelerated Gradient Descent

### Student Requests
+ May 25: Adagrad
+ May 27: Bayesian methods

### Project Presentations (Depending on Number of Projects)
+ Jun 01: Project (HW 5 due)
+ Jun 03: Project


+ Project report due on Jun 08



## Related Courses:
+ [Aaron Sidford, Introduction to Optimization Theory](https://web.stanford.edu/~sidford/courses/20fa_opt_theory/fa20_opt_theory.html)
+ [Lap Chi Lau, Convexity and Optimization](https://cs.uwaterloo.ca/~lapchi/cs798/index.html)
+ [Nisheeth Vishnoi, Algorithms for Convex Optimization](https://nisheethvishnoi.wordpress.com/convex-optimization/)
+ [Jonathan Kelner, Topics in Theoretical Computer Science: An Algorithmist's Toolkit](http://stellar.mit.edu/S/course/18/sp14/18.409/index.html)
+ [Santosh Vempala, Simple Algorithms](https://algorithms2017.wordpress.com/lectures/)
+ [Sheehan Olver, Numerical Complex Analysis](http://www.maths.usyd.edu.au/u/olver/teaching/NCA/)

## Other Lecture Notes
+ [Sébastien Bubeck, Convex Optimization: Algorithms and Complexity](https://arxiv.org/abs/1405.4980)
+ [Aharon Ben-Tal and Arkadi Nemirovski, Lectures on Modern Convex Optimization](https://www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf)
+ [Stephen J. Wright, Optimization Algorithms for Data Analysis](http://www.optimization-online.org/DB_FILE/2016/12/5748.pdf)
+ [Léon Bottou, Frank E. Curtis, Jorge Nocedal. Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838)

